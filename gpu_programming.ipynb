{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel computing and GPU programming with Julia \n",
    "## Part III: GPU programming\n",
    "Alexis Montoison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "using CUDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia has first-class support for GPU programming through the following packages that target GPUs from major vendors:\n",
    "- [CUDA.jl](https://github.com/JuliaGPU/CUDA.jl) for NVIDIA GPUs\n",
    "- [AMDGPU.jl](https://github.com/JuliaGPU/AMDGPU.jl) for AMD GPUs\n",
    "- [oneAPI.jl](https://github.com/JuliaGPU/oneAPI.jl) for Intel GPUs\n",
    "- [Metal.jl](https://github.com/JuliaGPU/Metal.jl) for Apple M-series GPUs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA.jl is the most mature and we will use it for the workshop.\n",
    "AMDGPU.jl is somewhat behind but still ready for general use, while oneAPI.jl and Metal.jl are functional but might contain bugs, miss some features and provide suboptimal performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference between a CPU and a GPU? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./Graphics/cpu_vs_gpu.png' width='700'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./Graphics/meme_gpu.jpg' width='300'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some key aspects of GPUs that need to be kept in mind:\n",
    "- The large number of compute elements on a GPU (in the thousands) can enable extreme scaling for data parallel tasks.\n",
    "- GPUs have their own memory. This means that data needs to be transfered to and from the GPU during the execution of a program.\n",
    "- Cores in a GPU are arranged into a particular structure. At the highest level they are divided into “streaming multiprocessors” (SMs). Some of these details are important when writing own GPU kernels."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gpu](./Graphics/gpu.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU programming with Julia can be as simple as using a different array type instead of regular `Base.Array` arrays:\n",
    "- `CuArray` from CUDA.jl for NVIDIA GPUs\n",
    "- `ROCArray` from AMDGPU.jl for AMD GPUs\n",
    "- `oneArray` from oneAPI.jl for Intel GPUs\n",
    "- `MtlArray` from Metal.jl for Apple GPUs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These array types are subtypes of `GPUArrays` from [GPUArrays.jl](https://github.com/JuliaGPU/GPUArrays.jl) and closely resemble `Base.Array` which enables us to write generic code which works on both CPU and GPU arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element CuArray{Int64, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if CUDA.functional()\n",
    "    A_d = CuArray([1,2,3,4])\n",
    "    A_d .+= 1\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same operation with other subtypes of `GPUArrays`:\n",
    "```julia\n",
    "if AMDGPU.functional()\n",
    "    A_d = ROCArray([1,2,3,4])\n",
    "    A_d .+= 1\n",
    "end\n",
    "\n",
    "if oneAPI.functional()\n",
    "    A_d = oneArray([1,2,3,4])\n",
    "    A_d .+= 1\n",
    "end\n",
    "\n",
    "A_d = MtlArray([1,2,3,4])\n",
    "A_d .+= 1\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving an array back from the GPU to the CPU is simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Int64}:\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if CUDA.functional()\n",
    "    A = Array(A_d)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the overhead of copying data to the GPU makes such simple calculations very slow."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s have a look at a more realistic example: matrix multiplication.\n",
    "We create two random arrays, one on the CPU and one on the GPU, and compare the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  573.211 ms (2 allocations: 128.00 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.666150 seconds (148 CPU allocations: 7.852 KiB) (1 GPU allocation: 128.000 MiB, 0.00% memmgmt time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4096×4096 CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}:\n",
       " 1032.67  1031.68  1028.22  1035.34  1034.16  …  1029.3   1030.93   1034.47\n",
       " 1002.6   1021.2   1010.54  1015.79  1015.36     1011.56  1001.06   1003.26\n",
       " 1025.8   1035.46  1019.12  1029.53  1022.36     1026.57  1019.43   1008.84\n",
       " 1023.68  1032.35  1017.77  1020.74  1028.01     1018.98  1027.65   1016.25\n",
       " 1020.99  1029.74  1007.86  1012.55  1015.62     1021.65  1013.08   1013.71\n",
       " 1032.27  1041.9   1023.99  1040.96  1028.83  …  1040.42  1038.84   1030.26\n",
       " 1024.15  1037.4   1010.27  1026.66  1026.66     1027.99  1034.4    1026.1\n",
       " 1013.42  1016.75  1003.63  1014.28  1005.89     1023.0    995.707  1008.09\n",
       " 1026.13  1028.38  1018.79  1029.14  1021.47     1025.18  1022.01   1014.15\n",
       " 1026.46  1025.74  1007.86  1024.72  1020.56     1019.32  1014.24   1010.99\n",
       "    ⋮                                         ⋱                        ⋮\n",
       " 1018.75  1018.06  1004.0   1020.67  1030.53     1026.18  1019.75   1013.39\n",
       " 1025.39  1028.22  1016.3   1030.17  1030.53     1026.78  1017.2    1014.6\n",
       " 1019.13  1035.2   1015.79  1038.02  1015.69     1029.15  1022.08   1026.62\n",
       " 1008.51  1016.89  1006.45  1015.69  1010.8   …  1020.34  1011.46    998.627\n",
       " 1019.55  1045.77  1010.09  1035.68  1021.49     1038.85  1035.0    1021.08\n",
       " 1031.18  1039.69  1023.49  1038.75  1032.73     1041.77  1019.8    1026.78\n",
       " 1033.67  1044.02  1018.96  1032.4   1022.38     1033.01  1025.38   1017.97\n",
       " 1009.07  1022.6   1015.78  1017.19  1011.5      1023.98  1017.48   1015.56\n",
       " 1031.78  1043.37  1031.51  1032.78  1038.38  …  1039.74  1032.23   1028.15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if CUDA.functional()\n",
    "    A = rand(2^12, 2^12)\n",
    "    A_d = CuArray(A)\n",
    "\n",
    "    @btime $A * $A\n",
    "    CUDA.@time A_d * A_d\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  273.121 ms (2 allocations: 64.00 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.233722 seconds (435.95 k CPU allocations: 22.972 MiB) (1 GPU allocation: 64.000 MiB, 0.01% memmgmt time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4096×4096 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n",
       " 1045.63  1054.6   1026.77   1045.75  1042.25  …  1030.96   1059.39  1044.02\n",
       " 1024.28  1032.75   997.311  1024.63  1013.51     1017.36   1019.15  1017.23\n",
       " 1034.15  1038.25  1007.79   1034.34  1022.76     1023.42   1037.81  1022.22\n",
       " 1030.53  1027.09   995.217  1011.15  1004.87     1003.04   1027.02  1006.19\n",
       " 1033.95  1041.93  1010.0    1039.49  1028.18     1028.47   1048.55  1025.83\n",
       " 1051.29  1053.68  1025.8    1041.63  1036.12  …  1044.43   1054.34  1033.68\n",
       " 1036.35  1038.97  1003.96   1031.6   1004.47     1022.85   1040.2   1021.7\n",
       " 1016.27  1038.13   998.653  1006.87  1000.37     1004.78   1037.67  1010.55\n",
       " 1041.17  1058.92  1017.04   1045.88  1026.89     1039.81   1045.81  1035.02\n",
       " 1034.44  1046.93  1000.6    1016.45  1008.62     1016.7    1024.59  1017.94\n",
       "    ⋮                                          ⋱                        ⋮\n",
       " 1036.96  1044.72  1012.24   1039.13  1021.15     1031.71   1051.52  1030.16\n",
       " 1049.45  1046.66  1024.33   1045.79  1032.86     1033.91   1048.64  1034.39\n",
       " 1016.46  1022.73   986.538  1013.26  1001.56      998.281  1017.92   995.238\n",
       " 1032.93  1029.31  1004.77   1035.44  1018.72  …  1022.97   1036.86  1035.74\n",
       " 1026.6   1032.91  1000.64   1027.54  1016.33     1030.0    1037.16  1022.46\n",
       " 1026.6   1038.02   998.769  1031.24  1015.62     1011.51   1028.64  1020.96\n",
       " 1023.09  1026.96   999.611  1020.63  1014.73     1016.29   1032.07  1019.8\n",
       " 1019.65  1031.8   1012.23   1036.28  1006.76     1014.79   1027.43  1019.09\n",
       " 1043.48  1051.41  1014.0    1043.81  1020.01  …  1019.53   1052.12  1025.07"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if CUDA.functional()\n",
    "    A = rand(Float32, 2^12, 2^12)\n",
    "    A_d = CuArray(A)\n",
    "    @btime $A * $A\n",
    "    CUDA.@time A_d * A_d\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPUs normally perform significantly better for 32-bit floats. Some GPUs doesn't support 64-bit floats!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many array operations in Julia are implemented using loops, processing one element at a time. Doing so with GPU arrays is very ineffective, as the loop won't actually execute on the GPU, but transfer one element at a time and process it on the CPU. As this wrecks performance, you will be warned when performing this kind of iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA.functional()\n",
    "    A_d[1] = 3.0\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scalar indexing is only allowed in an interactive session, e.g. the REPL, because it is convenient when porting CPU code to the GPU. If you want to disallow scalar indexing, e.g. to verify that your application executes correctly on the GPU, call the allowscalar function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA.functional()\n",
    "    CUDA.allowscalar(false)\n",
    "    A_d[1] = 3.0\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a non-interactive session, e.g. when running code from a script or application, scalar indexing is disallowed by default. There is no global toggle to allow scalar indexing; if you really need it, you can mark expressions using allowscalar with do-block syntax or `@allowscalar` macro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA.functional()\n",
    "    CUDA.allowscalar(false)\n",
    "\n",
    "    CUDA.allowscalar() do\n",
    "        A_d[1] += 1\n",
    "    end\n",
    "\n",
    "    CUDA.@allowscalar A_d[1] += 1\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nvidia provides CUDA toolkit, a collection of libraries that contain precompiled kernels for common operations like matrix multiplication ([cuBLAS](https://docs.nvidia.com/cuda/cublas/)), fast Fourier transforms ([cuFFT](https://docs.nvidia.com/cuda/cufft/)), linear solvers ([cuSOLVER](https://docs.nvidia.com/cuda/cusolver/)), sparse linear algebra ([CUSPARSE](https://docs.nvidia.com/cuda/cusparse/)), etc.\n",
    "These kernels are wrapped in CUDA.jl and can be used directly with CuArrays."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recommended way to use CUDA.jl is to let it automatically download an appropriate CUDA toolkit. CUDA.jl will check your driver's capabilities, which versions of CUDA are available for your platform, and automatically download an appropriate artifact containing all the libraries that CUDA.jl supports."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```julia\n",
    "CUDA.set_runtime_version!(v\"11.8\")\n",
    "```\n",
    "To use a local installation, you can invoke the same API but set the version to `\"local\"`:\n",
    "```julia\n",
    "CUDA.set_runtime_version!(\"local\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA runtime 11.8, artifact installation\n",
      "CUDA driver 12.1\n",
      "NVIDIA driver 530.30.2\n",
      "\n",
      "Libraries: \n",
      "- CUBLAS: 11.11.3\n",
      "- CURAND: 10.3.0\n",
      "- CUFFT: 10.9.0\n",
      "- CUSOLVER: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.4.1\n",
      "- CUSPARSE: 11.7.5\n",
      "- CUPTI: 18.0.0\n",
      "- NVML: 12.0.0+530.30.2\n",
      "\n",
      "Toolchain:\n",
      "- Julia: 1.8.5\n",
      "- LLVM: 13.0.1\n",
      "- PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4, 6.5, 7.0, 7.1, 7.2\n",
      "- Device capability support: sm_35, sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75, sm_80, sm_86\n",
      "\n",
      "1 device:\n",
      "  0: NVIDIA GeForce RTX 2070 Super (sm_75, 5.998 GiB / 8.000 GiB available)\n"
     ]
    }
   ],
   "source": [
    "if CUDA.functional()\n",
    "    CUDA.versioninfo()\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a guided tour of what is inside CUDA.jl!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA.functional()\n",
    "    using CUDA.CUBLAS\n",
    "    using CUDA.CUFFT\n",
    "    using CUDA.CUSOLVER\n",
    "    using CUDA.CUSPARSE\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A powerful way to program GPUs with arrays is through Julia’s higher-order array abstractions.\n",
    "The simple element-wise addition we saw above, `a .+= 1`, is an example of this, but more general constructs can be created with `broadcast`, `map`, `reduce`, `accumulate` etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096×4096 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n",
       " -0.723568   -0.543832   -0.0437073  …  -0.843247    -0.958513   -0.193473\n",
       " -0.534164   -0.0107928  -0.375073      -0.939359    -0.146964   -0.223677\n",
       " -0.0538156  -0.5883     -0.441566      -0.477366    -0.978755   -0.283492\n",
       " -0.759323   -0.765987   -0.3644        -0.75736     -0.466245   -0.347597\n",
       " -0.833889   -0.794054   -0.250753      -0.666238    -0.779951   -0.0234583\n",
       " -0.157873   -0.714707   -0.823822   …  -0.716101    -0.0476163  -0.78971\n",
       " -0.808679   -0.452048   -0.711343      -0.817588    -0.456819   -0.49476\n",
       " -0.922374   -0.980734   -0.6057        -0.0680697   -0.372029   -0.211417\n",
       " -0.19559    -0.430557   -0.947154      -0.210618    -0.853541   -0.396716\n",
       " -0.307655   -0.0204104  -0.0700517     -0.574448    -0.96172    -0.187183\n",
       "  ⋮                                  ⋱                            ⋮\n",
       " -0.952737   -0.999112   -0.352112      -0.642825    -0.422917   -0.956365\n",
       " -0.097131   -0.10814    -0.185954      -0.861275    -0.938897   -0.996842\n",
       " -0.831914   -0.0258279  -0.663557      -0.00509089  -0.823123   -0.804546\n",
       " -0.70208    -0.0811622  -0.522826   …  -0.657058    -0.524689   -0.253346\n",
       " -0.945246   -0.49871    -0.0982175     -0.659292    -0.451592   -0.212946\n",
       " -0.536527   -0.754618   -0.789032      -0.243517    -0.42048    -0.745087\n",
       " -0.288477   -0.4863     -0.325732      -0.87544     -0.656886   -0.297679\n",
       " -0.066795   -0.367851   -0.722515      -0.346786    -0.351121   -0.795294\n",
       " -0.348739   -0.587061   -0.471296   …  -0.195358    -0.740261   -0.475244"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if CUDA.functional()\n",
    "    broadcast(-, A_d, 1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096×4096 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n",
       " 1.27643  1.45617  1.95629  1.37037  …  1.0502   1.15675  1.04149  1.80653\n",
       " 1.46584  1.98921  1.62493  1.75959     1.49312  1.06064  1.85304  1.77632\n",
       " 1.94618  1.4117   1.55843  1.44545     1.54681  1.52263  1.02124  1.71651\n",
       " 1.24068  1.23401  1.6356   1.77889     1.33846  1.24264  1.53375  1.6524\n",
       " 1.16611  1.20595  1.74925  1.7153      1.01942  1.33376  1.22005  1.97654\n",
       " 1.84213  1.28529  1.17618  1.74993  …  1.01643  1.2839   1.95238  1.21029\n",
       " 1.19132  1.54795  1.28866  1.0058      1.61055  1.18241  1.54318  1.50524\n",
       " 1.07763  1.01927  1.3943   1.03687     1.29877  1.93193  1.62797  1.78858\n",
       " 1.80441  1.56944  1.05285  1.87457     1.23308  1.78938  1.14646  1.60328\n",
       " 1.69235  1.97959  1.92995  1.92466     1.56531  1.42555  1.03828  1.81282\n",
       " ⋮                                   ⋱                             ⋮\n",
       " 1.04726  1.00089  1.64789  1.82304     1.55603  1.35717  1.57708  1.04364\n",
       " 1.90287  1.89186  1.81405  1.43621     1.85639  1.13873  1.0611   1.00316\n",
       " 1.16809  1.97417  1.33644  1.58196     1.97995  1.99491  1.17688  1.19545\n",
       " 1.29792  1.91884  1.47717  1.60107  …  1.01021  1.34294  1.47531  1.74665\n",
       " 1.05475  1.50129  1.90178  1.09431     1.02914  1.34071  1.54841  1.78705\n",
       " 1.46347  1.24538  1.21097  1.43216     1.28116  1.75648  1.57952  1.25491\n",
       " 1.71152  1.5137   1.67427  1.93721     1.68069  1.12456  1.34311  1.70232\n",
       " 1.93321  1.63215  1.27749  1.13433     1.55997  1.65321  1.64888  1.20471\n",
       " 1.65126  1.41294  1.5287   1.34827  …  1.58506  1.80464  1.25974  1.52476"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if CUDA.functional()\n",
    "    map(x -> x+1, A_d)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.3885985f6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if CUDA.functional()\n",
    "    reduce(+, A_d)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096×4096 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n",
       "    0.276432  2065.51  4137.75  6143.16  …  8.38244f6  8.38448f6  8.38656f6\n",
       "    0.742269  2066.5   4138.37  6143.92     8.38244f6  8.38448f6  8.38656f6\n",
       "    1.68845   2066.91  4138.93  6144.36     8.38244f6  8.38448f6  8.38656f6\n",
       "    1.92913   2067.14  4139.57  6145.14     8.38244f6  8.38448f6  8.38656f6\n",
       "    2.09524   2067.35  4140.32  6145.86     8.38244f6  8.38448f6  8.38656f6\n",
       "    2.93737   2067.63  4140.49  6146.61  …  8.38244f6  8.38448f6  8.38656f6\n",
       "    3.12869   2068.18  4140.78  6146.61     8.38244f6  8.38448f6  8.38656f6\n",
       "    3.20632   2068.2   4141.18  6146.65     8.38244f6  8.38448f6  8.38656f6\n",
       "    4.01073   2068.77  4141.23  6147.53     8.38244f6  8.38448f6  8.38656f6\n",
       "    4.70307   2069.75  4142.16  6148.45     8.38244f6  8.38448f6  8.38656f6\n",
       "    ⋮                                    ⋱                        ⋮\n",
       " 2060.87      4131.7   6138.57  8196.79     8.38448f6  8.38655f6  8.3886f6\n",
       " 2061.77      4132.59  6139.38  8197.22     8.38448f6  8.38655f6  8.3886f6\n",
       " 2061.94      4133.57  6139.72  8197.8      8.38448f6  8.38655f6  8.3886f6\n",
       " 2062.24      4134.49  6140.2   8198.4   …  8.38448f6  8.38655f6  8.3886f6\n",
       " 2062.29      4134.99  6141.1   8198.5      8.38448f6  8.38655f6  8.3886f6\n",
       " 2062.76      4135.23  6141.31  8198.93     8.38448f6  8.38655f6  8.3886f6\n",
       " 2063.47      4135.75  6141.98  8199.87     8.38448f6  8.38655f6  8.3886f6\n",
       " 2064.4       4136.38  6142.26  8200.0      8.38448f6  8.38655f6  8.3886f6\n",
       " 2065.05      4136.79  6142.79  8200.35  …  8.38448f6  8.38655f6  8.3886f6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if CUDA.functional()\n",
    "    accumulate(+, A_d)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the high-level GPU array functionality made it easy to perform this computation on the GPU. However, we didn't learn about what's going on under the hood, and that's the main goal of this workshop. It's time to write our own kernels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vadd! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function vadd!(C, A, B)\n",
    "    for i in 1:length(A)\n",
    "        @inbounds C[i] = A[i] + B[i]\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Float64}:\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = ones(10)\n",
    "B = ones(10)\n",
    "C = similar(B)\n",
    "vadd!(C, A, B)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if CUDA.functional()\n",
    "    # We can already run this on the GPU with the @cuda macro,\n",
    "    # which will compile vadd!() into a GPU kernel and launch it\n",
    "    A_d = CuArray(A)\n",
    "    B_d = CuArray(B)\n",
    "    C_d = similar(B_d)\n",
    "    @cuda vadd!(C_d, A_d, B_d)\n",
    "    C_d\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The macros for the other GPU backends are `@roc`, `@oneapi` and `@metal`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance are just terrible because each thread on the GPU would be performing the same loop! So we have to remove the loop over all elements and instead use the special `threadIdx` and `blockDim` functions, analogous respectively to `threadid` and `nthreads` for multithreading."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can split work between the GPU threads by using a special function which returns the index of the GPU thread which executes it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vadd2! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function vadd2!(C, A, B)\n",
    "    index = threadIdx().x   # linear indexing, so only use `x`\n",
    "    @inbounds C[index] = A[index] + B[index]\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDA.HostKernel{typeof(vadd2!), Tuple{CuDeviceVector{Float32, 1}, CuDeviceVector{Float32, 1}, CuDeviceVector{Float32, 1}}}(vadd2!, CuFunction(Ptr{CUDA.CUfunc_st} @0x000000003ad87e00, CuModule(Ptr{CUDA.CUmod_st} @0x000000003d8c3a50, CuContext(0x0000000006d0c9f0, instance 2795a03b73c6d58a))), CUDA.KernelState(Ptr{Nothing} @0x00007f70a7200000))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if CUDA.functional()\n",
    "    N = 2^8\n",
    "    A = 2 * CUDA.ones(N)\n",
    "    B = 3 * CUDA.ones(N)\n",
    "    C = similar(B)\n",
    "\n",
    "    nthreads = N\n",
    "    @cuda threads=nthreads vadd2!(C, A, B)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if CUDA.functional()\n",
    "    all(Array(C) .== 5.0)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The syntax is similar for the other GPU backends!\n",
    "```julia\n",
    "groupsize = length(A)\n",
    "@roc groupsize=groupsize vadd!(C, A, B)\n",
    "\n",
    "items = length(A)\n",
    "@oneapi items=items vadd!(C, A, B)\n",
    "\n",
    "nthreads = length(A)\n",
    "@metal threads=nthreads vadd!(C, A, B)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do even better, we need to parallelize more. GPUs have a limited number of threads they can run on a single streaming multiprocessor (SM), but they also have multiple SMs. To take advantage of them all, we need to run a kernel with multiple blocks. We'll divide up the work like this:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gpu_threads_block](./Graphics/gpu_threads_block.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This diagram was borrowed from a description of the NVIDIA C/C++ library; in Julia, threads and blocks begin numbering with 1 instead of 0. In this diagram, the 4096 blocks of 256 threads (making 1048576 = 2^20 threads) ensures that each thread increments just a single entry; however, to ensure that arrays of arbitrary size can be handled, let's still use a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vadd3! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function vadd3!(C, A, B)\n",
    "    index = threadIdx().x + (blockIdx().x - 1) * blockDim().x\n",
    "    stride = gridDim().x * blockDim().x\n",
    "    for i = index:stride:length(B)\n",
    "        @inbounds C[index] = A[index] + B[index]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if CUDA.functional()\n",
    "    nthreads = CUDA.attribute(device(), CUDA.DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum number of allowed threads to launch depends on your GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if CUDA.functional()\n",
    "    N = 2^14\n",
    "    A = 2 * CUDA.ones(N)\n",
    "    B = 3 * CUDA.ones(N)\n",
    "    C = similar(B)\n",
    "\n",
    "    # smallest integer larger than or equal to N / nthreads\n",
    "    numblocks = ceil(Int, N/nthreads)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDA.HostKernel{typeof(vadd3!), Tuple{CuDeviceVector{Float32, 1}, CuDeviceVector{Float32, 1}, CuDeviceVector{Float32, 1}}}(vadd3!, CuFunction(Ptr{CUDA.CUfunc_st} @0x000000003a981750, CuModule(Ptr{CUDA.CUmod_st} @0x000000003980b560, CuContext(0x0000000006d0c9f0, instance 2795a03b73c6d58a))), CUDA.KernelState(Ptr{Nothing} @0x00007f70a7200000))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if CUDA.functional()\n",
    "    @cuda threads=nthreads blocks=numblocks vadd3!(C, A, B)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " all(Array(C) .== 5.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA.jl supports indexing in up to 3 dimensions (x, y and z, e.g. `threadIdx().z`). This is convenient for multidimensional data where thread blocks can be organised into 1D, 2D or 3D arrays of threads."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To automatically select an appropriate number of threads, it is recommended to use the launch configuration API. This API takes a compiled (but not launched) kernel, returns a tuple with an upper bound on the number of threads, and the minimum number of blocks that are required to fully saturate the GPU:\n",
    "\n",
    "To optimize the number of threads, we can first create the kernel without launching it, query it for the number of threads supported, and then launch the compiled kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "16"
     ]
    }
   ],
   "source": [
    "# compile kernel\n",
    "kernel = @cuda launch=false vadd3!(C, A, B)\n",
    "\n",
    "# extract configuration via occupancy API\n",
    "config = launch_configuration(kernel.fun)\n",
    "\n",
    "# number of threads should not exceed size of array\n",
    "threads = min(length(A), config.threads)\n",
    "print(threads,'\\n')\n",
    "\n",
    "# smallest integer larger than or equal to length(A)/threads\n",
    "blocks = cld(length(A), threads)\n",
    "print(blocks,'\\n')\n",
    "\n",
    "# launch kernel with specific configuration\n",
    "kernel(C, A, B; threads, blocks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Debugging**: Many things can go wrong with GPU kernel programming and unfortunately error messages are sometimes not very useful because of how the GPU compiler works.\n",
    "\n",
    "Conventional print-debugging is often a reasonably effective way to debug GPU code. CUDA.jl provides macros that facilitate this:\n",
    "- `@cushow` (like @show): visualize an expression and its result, and return that value.\n",
    "- `@cuprintln` (like println): to print text and values.\n",
    "- `@cuaassert` (like @assert) can also be useful to find issues and abort execution.\n",
    "\n",
    "GPU code introspection macros also exist, like `@device_code_warntype`, to track down type instabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread 1, block 10\n",
      "thread 2, block 10\n",
      "thread 3, block 10\n",
      "thread 4, block 10\n",
      "thread 5, block 10\n",
      "thread 6, block 10\n",
      "thread 7, block 10\n",
      "thread 8, block 10\n",
      "thread 9, block 10\n",
      "thread 10, block 10\n"
     ]
    }
   ],
   "source": [
    "function gpu_add_print!(y, x)\n",
    "    index = threadIdx().x    # this example only requires linear indexing, so just use `x`\n",
    "    stride = blockDim().x\n",
    "    @cuprintln(\"thread $index, block $stride\")\n",
    "    for i = index:stride:length(y)\n",
    "        @inbounds y[i] += x[i]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "if CUDA.functional()\n",
    "    x_d = CUDA.rand(10)\n",
    "    y_d = CUDA.rand(10)\n",
    "    @cuda threads=10 gpu_add_print!(y_d, x_d)\n",
    "    synchronize()\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: Keep in mind that the high-level functionality of CUDA often means that you don't need to worry about writing kernels at such a low level. However, there are many cases where computations can be optimized using clever low-level manipulations. The kernels implemented in Julia give you all the flexibility and performance a GPU has to offer, within a familiar language.\n",
    "\n",
    "A typical approach for porting or developing an application for the GPU is as follows:\n",
    "- develop an application using generic array functionality, and test it on the CPU with the `Array` type;\n",
    "- port your application to the GPU by switching to the `CuArray` type;\n",
    "- disallow the CPU fallback (\"scalar indexing\") to find operations that are not implemented for or incompatible with GPU execution;\n",
    "- (optional) use lower-level, CUDA-specific interfaces to implement missing functionality or optimize performance.   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: GPU-port the `sqrt_sum` function we saw in te first notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sqrt_sum(A)\n",
    "    T = eltype(A)\n",
    "    s = zero(T)\n",
    "    for i in eachindex(A)\n",
    "        @inbounds s += sqrt(A[i])\n",
    "    end\n",
    "    return s\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "- https://enccs.github.io/Julia-for-HPC/GPU/\n",
    "- https://cuda.juliagpu.org/stable/\n",
    "- https://www.youtube.com/watch?v=Fz-ogmASMAE\n",
    "- https://www.cherryservers.com/blog/gpu-vs-cpu-what-are-the-key-differences\n",
    "- https://developer.nvidia.com/blog/tag/cuda-refresher/\n",
    "- https://i.redd.it/yr9h5cpyzpn21.jpg\n",
    "- https://docs.nvidia.com/cuda/\n",
    "- https://www.youtube.com/watch?v=Hz9IMJuW5hU\n",
    "- https://julialang.org/learning/"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
